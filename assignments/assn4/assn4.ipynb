{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkGwXwzD2-MK"
      },
      "source": [
        "# Intermediate Machine Learning: Assignment 4\n",
        "\n",
        "**Deadline**\n",
        "\n",
        "Assignment 4 is due Monday, November 18 by 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on Canvas).\n",
        "\n",
        "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
        "\n",
        "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
        "\n",
        "**Submission**\n",
        "\n",
        "Submit your assignment as a pdf file on Gradescope, and as a notebook (.ipynb) on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to more easily find your complete solution to each problem.\n",
        "\n",
        "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:\n",
        "\n",
        "Go to \"File\" at the top-left of your Jupyter Notebook\n",
        "Under \"Download as\", select \"HTML (.html)\"\n",
        "After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
        "From the print window, select the option to save as a .pdf\n",
        "\n",
        "**Topics**\n",
        "\n",
        " * Graph kernels\n",
        " * Reinforcement learning\n",
        "\n",
        "This assignment will also help to solidify your Python skills."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uHcIjQl2-MM"
      },
      "source": [
        "$\\renewcommand{\\reals}{{\\mathbb R}}\n",
        "\\newcommand{\\indp}{\\perp\\kern-4pt\\perp}\n",
        "\\newcommand{\\given}{\\,|\\,}$\n",
        "\n",
        "\n",
        "## Problem 1: Graph kernels (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p55_F3xv2-MN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "import sklearn\n",
        "import random\n",
        "from numpy.linalg import inv\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fr2VmwK_2-MN"
      },
      "outputs": [],
      "source": [
        "# Helper functions for third part of exercise\n",
        "\n",
        "def rgb2gray(rgb):\n",
        "    \"\"\"Function to turn RGB images in greyscale images.\"\"\"\n",
        "    return np.dot(rgb[..., :3], [0.2989, 0.5870, 0.1140])\n",
        "\n",
        "\n",
        "def grid_adj(rows, cols):\n",
        "    \"\"\"Function that creates the adjacency matrix of\n",
        "    a grid graph with predefined amount of rows and columns.\"\"\"\n",
        "    M = np.zeros([rows*cols, rows*cols])\n",
        "    for r in np.arange(rows):\n",
        "        for c in np.arange(cols):\n",
        "            i = r*cols + c\n",
        "            if c > 0:\n",
        "                M[i-1, i] = M[i, i-1] = 1\n",
        "            if r > 0:\n",
        "                M[i-cols, i] = M[i, i-cols] = 1\n",
        "    return M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDeMagDJ2-MO"
      },
      "source": [
        "The graph Laplacian for a weighted graph on $n$ nodes is defined as\n",
        "\n",
        "$$ L = D - W$$\n",
        "\n",
        "where $W$ is an $n\\times n$ symmetric matrix of positive edge weights,\n",
        "with $W_{ij} = 0$ if $(i,j)$ is not an edge in the graph,\n",
        "and $D$ is the diagonal matrix with $D_{ii} = \\sum_{j=1}^n W_{ij}$.\n",
        "This generalizes the definition of the Laplacian\n",
        "used in class, where all of the edge weights are one.\n",
        "\n",
        "\n",
        "1. Show that $L$ is a Mercer kernel, by showing that $L$ is\n",
        "  symmetric and positive-semidefinite.\n",
        "<br>\n",
        "\n",
        "2. In graph neural networks we define polynomial filters of the form\n",
        "\n",
        "  $$ P = a_0 I + a_1 L + a_2 L^2 + \\cdots a_d L^d$$\n",
        "  \n",
        "  where $L$ is the Laplacian and $a_0,\\ldots, a_d$ are parameters,\n",
        "  corresponding to the filter parameters in standard convolutional\n",
        "  neural networks.\n",
        "\n",
        "  If each $a_i \\geq 0$ is non-negative, show that $P$ is also\n",
        "  a Mercer kernel.\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS0fYiXx2-MO"
      },
      "source": [
        "3. This polynomial filter has many applications. A handful of these applications\n",
        "   are based on the fact that, given a graph with a signal x, the value of $x^T L x$\n",
        "   will be low in case the signal is smooth (i.e. smooth transitions of x between\n",
        "   neighboring nodes). A large $x^T L x$ means that we have a rough graph signal (i.e.\n",
        "   a lot of jumps in x between neighboring nodes).\n",
        "   \n",
        "   An intersting application that uses this property is the so-called image inpainting\n",
        "   process, where an image is seen as grid graph. Image inpainting tries to restore\n",
        "   a corrupted image by smoothing out the neighboring pixel values. In this problem\n",
        "   we corrupt an image by turning off (i.e. making the pixel value equal to zero) a\n",
        "   certain portion of the pixels. Your goal will be to restore the corrupted image\n",
        "   and hence recreate the original image.\n",
        "   \n",
        "   First, let's corrupt an image by turning off a portion of the pixels. For this\n",
        "   exercise, we choose to turn off 30% of the pixels. The result is shown below.\n",
        "   Try to understand the code, as some variables might be interesting for your work.\n",
        "   \n",
        "   The image \"Yale_Bulldogs.jpg\" can be found in Canvas under assn4 folder, and also in the GitHub repo https://github.com/YData123/sds365-fa24/tree/main/assignments/assn4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0GZBdHbM2-MO",
        "outputId": "943ba304-3229-4d63-b998-f45f0e300de1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/Users/tadcarney/Downloads/Yale_Bulldogs.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0344fb606aae>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Normalize the pixels of the original image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/tadcarney/Downloads/Yale_Bulldogs.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Turn picture into greyscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgray_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb2gray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mheight_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgray_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0;34m\"``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m             )\n\u001b[0;32m-> 1525\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimg_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m         return (_pil_png_to_float_array(image)\n\u001b[1;32m   1527\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImagePlugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImageFile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3468\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3469\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3470\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3471\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/tadcarney/Downloads/Yale_Bulldogs.jpg'"
          ]
        }
      ],
      "source": [
        "# Normalize the pixels of the original image\n",
        "image = img.imread(\"/Users/tadcarney/Downloads/Yale_Bulldogs.jpg\") / 255\n",
        "# Turn picture into greyscale\n",
        "gray_image = rgb2gray(image)\n",
        "height_img = gray_image.shape[0]\n",
        "width_img = gray_image.shape[1]\n",
        "\n",
        "# Turn off (value 0) certain pixels\n",
        "fraction_off = int(0.30*height_img*width_img)\n",
        "mask = np.ones(height_img*width_img, dtype=int)\n",
        "# Set the first fraction of pixels off\n",
        "mask[:fraction_off] = 0\n",
        "# Shuffle to create randomness\n",
        "np.random.shuffle(mask)\n",
        "# Multiply the original image by the reshapes mask\n",
        "mask = np.reshape(mask, (height_img, width_img))\n",
        "corrupted_image = np.multiply(mask, gray_image)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "ax1.imshow(gray_image, cmap=plt.get_cmap('gray'))\n",
        "ax1.set_title(\"Original Image\")\n",
        "ax2.imshow(corrupted_image, cmap=plt.get_cmap('gray'))\n",
        "ax2.set_title(\"Corrupted Image\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqWcIVLQ2-MO"
      },
      "source": [
        "Inpainting missing pixel values can be formulated as the following\n",
        "optimization problem:\n",
        "\n",
        "$$\n",
        "\\underset{\\textbf{x} \\in \\mathbb{R}^n}{\\min} \\left\\{ \\lVert \\textbf{y} − \\textbf{M} \\textbf{x} \\rVert^2_2\n",
        "+ \\alpha \\textbf{x}^T\\textbf{P}\\textbf{x} \\right\\}\n",
        "$$\n",
        "\n",
        "where $\\textbf{y} \\in \\mathbb{R}^n$ ($n$ being the total amount of pixels) is the corrupted graph signal\n",
        "(with missing pixel values being 0) and $\\alpha$ is a regularization\n",
        "(smoothing) parameter that controls for smoothness of the graph. $\\textbf{P}$ is the\n",
        "polynomial filter based on the laplacian $\\textbf{L}$. Finally, $\\textbf{M}$\n",
        "$\\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix that satisfies:\n",
        "\n",
        "$$\n",
        "\\textbf{M}(i, i) =\n",
        "    \\begin{cases}\n",
        "      1, \\ \\textrm{if $\\textbf{y}$(i) is observed} \\\\ \\\\\n",
        "      0, \\ \\textrm{if $\\textbf{y}$(i) is corrupted}\n",
        "    \\end{cases}\\,\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGozZ0_A2-MO"
      },
      "source": [
        "The optimization problem tries to find an $\\textbf{x}$ that matches the\n",
        "observed values in $\\textbf{y}$, and at the same time tries to be smooth\n",
        "on the graph. Start with deriving a closed form solution of this optimization\n",
        "problem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz6bYD2_2-MP"
      },
      "outputs": [],
      "source": [
        "# your markdown here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHBCvl5P2-MP"
      },
      "source": [
        "Next, let's restore our image. To keep things simple, let's say we already\n",
        "trained the polynomial filter $\\textbf{P}$ of degree 2 and we found the\n",
        "following weights:\n",
        "\n",
        "$$\n",
        "\\textbf{P} =  \\textbf{L} + 0.05 \\ \\textbf{L}^2\n",
        "$$\n",
        "\n",
        "Fill in the following lines of code and show your reconstructed images next\n",
        "to the corrupted image. Assume that the weights on the graph edges are equal to 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXqdPNfD2-MP"
      },
      "outputs": [],
      "source": [
        "# Corrupted graph signal\n",
        "y =\n",
        "\n",
        "# Diagonal matrix defined as above\n",
        "M =\n",
        "\n",
        "# Adjacency matrix of the graph (by using the helper function)\n",
        "A =\n",
        "\n",
        "# Diagonal matrix defined as above\n",
        "D =\n",
        "\n",
        "# Graph Laplacian defined as above\n",
        "L =\n",
        "\n",
        "# Polynomial filter defined as above\n",
        "P ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG4U1hFL2-MP"
      },
      "outputs": [],
      "source": [
        "# closed form solution you derived above\n",
        "x ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LLXtRDF2-MP"
      },
      "outputs": [],
      "source": [
        "# Try to experiment with different alpha values\n",
        "alpha =\n",
        "reconstructed_image = np.reshape(x, (height_img, width_img))\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "ax1.imshow(corrupted_image, cmap=plt.get_cmap('gray'))\n",
        "ax1.set_title(\"Corrupted Image\")\n",
        "ax2.imshow(reconstructed_image, cmap=plt.get_cmap('gray'))\n",
        "ax2.set_title(\"Reconstructed Image\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWqDiHR92-MP"
      },
      "source": [
        "4. Discuss the influence of the smoothing parameter $\\alpha$ in\n",
        "   the optimization problem above. What happens for very large\n",
        "   and very low values of $\\alpha$? Finally, discuss the degree\n",
        "   of our polynomial function $\\textbf{P}$. What happens if\n",
        "   we would choose a large degree?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qql7iWa62-MP"
      },
      "outputs": [],
      "source": [
        "# your markdown and code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtaP5c6x2-MP"
      },
      "source": [
        "\n",
        "##  Problem 2:  Positive reinforcement  (10 points)\n",
        "$\\def\\J{{J}}$\n",
        "$\\def\\E{{\\mathbb E}}$\n",
        "\n",
        "As discussed in class, reinforcement learning\n",
        "using policy gradient methods is based on maximizing the\n",
        "expected total reward\n",
        "\n",
        "$$ \\J(\\theta) = \\E_\\theta [R(\\tau)],$$\n",
        "\n",
        "where the expectation is over the probability distribution over sequences $\\tau$ through a choice of actions using the policy. This can be rewritten as\n",
        "\n",
        "\\begin{align*}\n",
        "  \\nabla_\\theta \\J(\\theta) & = \\E_\\theta\\left[ R(\\tau) \\nabla_\\theta \\log p(\\tau\\given \\theta) \\right].\n",
        "\\end{align*}\n",
        "\n",
        "Approximating this gradient involves computing $\\nabla_\\theta \\log \\pi_\\theta (a \\given s)$ where $\\pi_\\theta$ is the policy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNXSQfqT2-MP"
      },
      "source": [
        "### 2.1 Continuous action space with Gaussian policy\n",
        "\n",
        "Suppose that the action space is continuous\n",
        "and $\\pi_\\theta(a\\given s)$ is a normal density with mean\n",
        "$\\mu_\\theta(s)$ and variance $\\sigma^2_\\theta(s)$, two outputs of\n",
        "a neural network with input $s$ and parameters $\\theta$.\n",
        "\n",
        "\n",
        "Suppose the outputs of the neural network are given by\n",
        "\n",
        "\\begin{align*}\n",
        "  \\mu_\\theta(s) & = \\beta_1^T h(s) \\\\\n",
        "  \\sigma^2_\\theta(s) &= \\text{exp}(\\beta_2^T h(s))\n",
        "\\end{align*}\n",
        "\n",
        "where $h(s)$ is the vector of neurons in the last layer, immediately\n",
        "before the outputs. Derive explicit expressions for\n",
        "$\\nabla_{\\beta_1} \\log \\pi_\\theta(a\\given s)$ and\n",
        "$\\nabla_{\\beta_2} \\log \\pi_\\theta(a\\given s)$.\n",
        "\n",
        "Explain how these gradients and other gradient\n",
        "terms in $\\nabla_\\theta \\log \\pi_\\theta(a\\given s)$ are used\n",
        "to estimate the policy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iy5X66I2-MP"
      },
      "source": [
        "### 2.2 Discrete action space with Softmax policy\n",
        "\n",
        "Suppose the action space is discrete with K possible actions, and the policy $\\pi_\\theta(a \\mid s)$ is defined using a softmax function over preferences $u_\\theta(s,a)$:\n",
        "\n",
        "$$\\pi_\\theta(a \\mid s) = \\frac{\\exp(u_\\theta(s,a))}{\\sum_a \\exp(u_\\theta(s,a))},$$\n",
        "where $u_\\theta(s,a) = \\beta^T h(s,a)$, and $h(s,a)$ is a feature vector for state-action pair $(s,a)$. Derive the expression for $\\nabla_\\beta \\log \\pi_\\theta(a \\mid s)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJLrUObx2-MP"
      },
      "source": [
        "## Problem 3: Deep Q-Learning for Flappy Bird (25 points)\n",
        "\n",
        "Deep Q-learning was proposed (and patented) by DeepMind and made\n",
        "a big splash when the same deep neural network architecture was shown to be able to surpass\n",
        "human performance on many different Atari games, playing directly from the pixels.\n",
        "In this problem, we will walk you through the implementation of deep Q-learning\n",
        "to learn to play the Flappy Bird game.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/YData123/sds365-sp22/main/assignments/assn4/images/flappy_bird_demp.gif\" width=\"144\" height=\"256\"/>\n",
        "\n",
        "The implementation is based these references:\n",
        "- [DeepLearningFlappyBird](https://github.com/yenchenlin/DeepLearningFlappyBird)\n",
        "- [Deep Q-Learning for Atari Breakout](https://keras.io/examples/rl/deep_q_network_breakout/)\n",
        "\n",
        "We use the `pygame` package to visualize the interaction between the algorithm\n",
        "and the game environment.\n",
        "However, _pygame_ is not well supported by Google Colab;\n",
        "we recommend you to run the code for this problem locally.\n",
        "A window will be popped up that displays\n",
        "the game as it progress in real-time (as for the Cartpole demo from class).\n",
        "\n",
        "This problem is structured as follows:\n",
        "\n",
        "* Load necessary packages\n",
        "* Test the visualization of the game, to make sure everything's working\n",
        "* Process the images to reduce the dimension\n",
        "* Setup the game history buffer\n",
        "* Implement the core Q-learning function\n",
        "* Run the learning algorithm\n",
        "* Interpret the results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBYBqb5r2-MP"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OShSsY2u2-MQ"
      },
      "source": [
        "The Flappy Bird game is requires a few Python packages. Please install these _as soon as possible_, and notify us of any issues you experience so that we can help. The Python files can also be found on Canvas and in our GitHub repo at https://github.com/YData123/sds365-fa24/tree/main/assignments/assn4 ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdQPedeo2-MQ",
        "outputId": "2c4dc7ce-e94f-4bf6-e4ba-e92dbfd89872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.1.2 (SDL 2.0.18, Python 3.8.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n"
          ]
        }
      ],
      "source": [
        "# %pip install pygame\n",
        "# %pip install opencv-python\n",
        "import numpy as np\n",
        "import cv2\n",
        "import wrapped_flappy_bird as flappy_bird\n",
        "from collections import deque\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, initializers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MkQssKP2-MQ"
      },
      "source": [
        "### The Flappy Bird environment\n",
        "\n",
        "Interaction with the game environment is carried out through calls of the form\n",
        "\n",
        "`(image, reward, terminal) = game.frame_step(action)`\n",
        "\n",
        "where the meaning of these variables is as follows:\n",
        "\n",
        "- `action`: $\\binom{1}{0}$ for doing nothing, $\\binom{0}{1}$ for \"flapping the bird's wings\"\n",
        "- `image`: the image for the next step of the game, of size $(288, 512, 3)$ with three RGB channels\n",
        "- `reward`: the reward received for taking the action; -1 if an obstacle is hit, 0.1 otherwise.\n",
        "- `terminal`: `True` if an obstacle is hit, otherwise `False`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCenPEmj2-MQ"
      },
      "source": [
        "Now let's take a look at the game interface.\n",
        "First, initiate the game:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeMAmwns2-MQ",
        "outputId": "ebb37d62-95fd-483d-b7f5-cc5c42a28bde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of image: (288, 512, 3)\n",
            "reward:  0.1\n",
            "terminal:  False\n"
          ]
        }
      ],
      "source": [
        "num_actions = 2\n",
        "\n",
        "# initiate a game\n",
        "game = flappy_bird.GameState()\n",
        "\n",
        "# get the first state by doing nothing\n",
        "do_nothing = np.zeros(num_actions)\n",
        "do_nothing[0] = 1\n",
        "image, reward, terminal = game.frame_step(do_nothing)\n",
        "\n",
        "print('shape of image:', image.shape)\n",
        "print('reward: ', reward)\n",
        "print('terminal: ', terminal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6APw6272-MQ"
      },
      "source": [
        "After running the above cells, a window should pop up, and you can watch the game being played in that window.\n",
        "\n",
        "Let's take some random actions and see what happens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDc2o36R2-MQ"
      },
      "outputs": [],
      "source": [
        "for i in range(587):\n",
        "\n",
        "    # choose a random action\n",
        "    action = np.random.choice(num_actions)\n",
        "\n",
        "    # create the corresponding one-hot vector\n",
        "    action_vec = np.zeros(num_actions)\n",
        "    action_vec[action] = 1\n",
        "\n",
        "    # take the action and observe the reward and the next state\n",
        "    image, reward, terminal = game.frame_step(action_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGHfoIFS2-MQ"
      },
      "source": [
        "Are you able to see Flappy moving across the window and crashing into things? Great! If you're\n",
        "having any issues, post to EdD and we'll do our best to help you out.\n",
        "\n",
        "Here is how we can visualize a frame of the game as an image within a cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4b746Zy2-MQ",
        "outputId": "02e1416b-566e-47f8-c329-ad7e804bb66e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# show the image\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(image.transpose([1, 0, 2]))\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXL5GSEf2-MQ"
      },
      "source": [
        "### Preprocessing the images\n",
        "\n",
        "Alright, next we need to prepocess the images by converting them to grayscale and resizing them to $80\\times 80$ pixels. This will help\n",
        "to reduce the computation, and aid learning. Besides, Flappy is\n",
        "\"color blind.\" (Fun fact: The instructor of this course is also\n",
        "[color vision deficient](https://en.wikipedia.org/wiki/Color_blindness).)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag27QtBb2-MQ",
        "outputId": "a2252f55-0eaf-431d-8ecf-9157f6b18aa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the transformed image: (288, 512, 3)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def resize_gray(frame):\n",
        "    frame = cv2.cvtColor(cv2.resize(frame, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
        "    ret, frame = cv2.threshold(frame, 1, 255, cv2.THRESH_BINARY)\n",
        "    return np.reshape(frame, (80, 80, 1))\n",
        "\n",
        "image_transformed = resize_gray(image)\n",
        "print('Shape of the transformed image:', image.shape)\n",
        "\n",
        "# show the transformed image\n",
        "_ = plt.imshow(image_transformed.transpose((1, 0, 2)), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mmL7nCC2-MR"
      },
      "source": [
        "This shows the preprocessed image for a single frame of\n",
        "the game. In our implementation of Deep Q-Learning, we encode the state by stacking four consecutive frames, resulting in\n",
        "a tensor of shape (80,80,4).\n",
        "\n",
        "Then, given the `current_state`, and a raw image `image_raw`\n",
        "of size $288\\times512\\times3$, we convert\n",
        "the raw image to a $80\\times80\\times 1$ grayscale image using the\n",
        "code in the previous cell. The ,\n",
        "we remove the first frame of `current_state` and add\n",
        "the new frame, giving again a stack of images of\n",
        "size (80, 80, 4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M33fUY-72-MR"
      },
      "outputs": [],
      "source": [
        "def preprocess(image_raw, current_state=None):\n",
        "    # resize and convert to grayscale\n",
        "    image = resize_gray(image_raw)\n",
        "    # stack the frames\n",
        "    if current_state is None:\n",
        "        state = np.concatenate((image, image, image, image), axis=2)\n",
        "    else:\n",
        "        state = np.concatenate((image, current_state[:, :, :3]), axis=2)\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAEZlXRr2-MR"
      },
      "source": [
        "### 3.1 Explain the game state\n",
        "\n",
        "Why is the state chosen to be a stack of four consecutive\n",
        "frames rather than a single frame? Give an intuitive explanation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkmXbJNJ2-MR"
      },
      "source": [
        "[your answer here in Markdown]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvetDgSk2-MR"
      },
      "source": [
        "###  Constructing the neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjTiK3wB2-MR"
      },
      "source": [
        "Now we are ready to construct the neural network for approximating the Q function. Recall that, given input $s$ which is of size $80\\times80\\times4$ due to the\n",
        "previous preprocessing, the output of the network should be of size 2, corresponding to the values of $Q(s,a_1)$ and $Q(s, a_2)$ respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV2hhN_22-MR"
      },
      "source": [
        "Here is the summary of the model we'd like to build:\n",
        "\n",
        "![Neural network](https://raw.githubusercontent.com/YData123/sds365-sp22/main/assignments/assn4/images/q_model.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRVcNRou2-MV"
      },
      "source": [
        "### 3.2 Initialize the network\n",
        "\n",
        "Complete the code in the next cell so that your model architecture matches that in the above picture. Here we specify the initialization of the weights by using `keras.initializers`.\n",
        "Note that we haven't talked about the `strides` argument for CNNs;\n",
        "you can read about stride here: [https://machinelearningmastery.com/padding-and-stride-for-convolutional-neural-networks/](https://machinelearningmastery.com/padding-and-stride-for-convolutional-neural-networks/). It's not important to understand this in detail, you just need to choose the number and sizes of the filters to get the shapes to match the specification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGE09fzA2-MV"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import initializers\n",
        "def create_q_model():\n",
        "    state = layers.Input(shape=(80, 80, 4,))\n",
        "\n",
        "    layer1 = layers.Conv2D(filters=..., kernel_size=..., strides=4, activation=\"relu\",\n",
        "                           kernel_initializer=initializers.TruncatedNormal(mean=0., stddev=0.01),\n",
        "                           bias_initializer=initializers.Constant(0.01))(state)\n",
        "    layer2 = layers.MaxPool2D(..., strides=2, padding=\"SAME\")(layer1)\n",
        "    layer3 = layers.Conv2D(filters=..., kernel_size=..., strides=2, activation=\"relu\",\n",
        "                           kernel_initializer=initializers.TruncatedNormal(mean=0., stddev=0.01),\n",
        "                           bias_initializer=initializers.Constant(0.01))(layer2)\n",
        "    layer4 = layers.Flatten()(layer3)\n",
        "    q_value = layers.Dense(units=..., activation=\"linear\",\n",
        "                           kernel_initializer=initializers.TruncatedNormal(mean=0., stddev=0.01),\n",
        "                           bias_initializer=initializers.Constant(0.01))(layer4)\n",
        "\n",
        "    return keras.Model(inputs=state, outputs=q_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FVYPlwZ2-MV"
      },
      "source": [
        "Plot the model summary to make sure that the network is the same as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DBI2aio2-MV"
      },
      "outputs": [],
      "source": [
        "model = create_q_model()\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3NIeZlM2-MV"
      },
      "source": [
        "### Deep Q-learning\n",
        "\n",
        "We're now ready to implement the Q-learning algorithm.\n",
        "There are some subtle details in the implementation that you need to sort out. First, recall that the update rule for Q learning is\n",
        "\n",
        "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r(s,a) + \\gamma\\cdot \\max_{a'} Q(\\text{next}(s,a), a') - Q(s,a))$$\n",
        "\n",
        "where $\\gamma$ is the discount factor and $\\alpha$ can be viewed as the step size or learning rate for gradient ascent.\n",
        "\n",
        "We'll set these as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSeAXMTl2-MV"
      },
      "outputs": [],
      "source": [
        "gamma = 0.99            # decay rate of past observations\n",
        "step_size = 1e-4        # step size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd7cu4RA2-MV"
      },
      "source": [
        "### Estimation with experience replay\n",
        "\n",
        "At the beginning of training, we spend 10,000 steps taking random\n",
        "actions, as a means of observing the environment.\n",
        "\n",
        "We build a replay memory of length 10,000 steps, and every time we update the weights of the network, we sample a batch of size 32 and perform a Q-learning update on this batch.\n",
        "\n",
        "After we have collected 10,000 steps of new data, we discard\n",
        "the old data, and replace it with the new \"experiences.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpcfLX-B2-MV"
      },
      "outputs": [],
      "source": [
        "observe = 10000            # timesteps to observe before training\n",
        "replay_memory = 10000      # number of previous transitions to remember\n",
        "batch_size = 32            # size of each batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhcvqQuY2-MV"
      },
      "source": [
        "\n",
        "### 3.3 Justify the data collection\n",
        "\n",
        "Why does it make sense to maintain the replay memory of a fixed size\n",
        "instead of including all of the historical data?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw72CZR62-MV"
      },
      "source": [
        "[Your answer here in Markdown]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMrvVJcw2-MV"
      },
      "source": [
        "### Exploration vs exploitation\n",
        "\n",
        "When performing Q-learning, we face the tradeoff between exploration and\n",
        "exploitation.  To encourage exploration, a simple strategy is to take a random action at each step with certain probability.\n",
        "\n",
        "More precisely, for each time step $t$ and state $s_t$, with probability $\\epsilon$, the algorithm takes a random action (wing flap or do nothing), and with probability $1-\\epsilon$ the\n",
        "algorithm takes a greedy action according to $a_t = \\arg\\max_{a} Q_\\theta(s_t,a)$. Here $\\theta$ refers to the parameters of our CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnPAf-l72-MV"
      },
      "outputs": [],
      "source": [
        "# value of epsilon\n",
        "epsilon = 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytXZYhgT2-MV"
      },
      "source": [
        "### 3.4 Complete the Q-learning algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efd1hHGM2-MV"
      },
      "source": [
        "Next you will need to complete the Q-learning algorithm by filling in the missing code in the following function.\n",
        "The missing parts include\n",
        "\n",
        "- Taking a greedy action\n",
        "- Given a batch of samples $\\{(s_t, a_t, r_t, s_{t+1}, \\text{terminal}_t)\\}_{t\\in B}$, computing the corresponding $Q_\\theta(s_t, a_t)$.\n",
        "- Given a batch of samples $\\{(s_t, a_t, r_t, s_{t+1}, \\text{terminal}_t)\\}_{t\\in B}$, computing the corresponding updated Q-values\n",
        "  \n",
        "$$\\hat{y}(s_t,a_t) = \\begin{cases}\n",
        "r_t + \\gamma\\, \\max_{a} Q_\\theta(s_{t+1}, a), & \\text{if } \\text{terminal}_t=0,\\\\\n",
        "r_t, & \\text{otherwise}.\n",
        "\\end{cases}$$\n",
        "\n",
        "Then, the mean squared error loss for the batch is\n",
        "\n",
        "$$\\frac{1}{|B|} \\sum_{t\\in B} (\\hat y(s_t, a_t) - Q_\\theta(s_t, a_t))^2.$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPiWrGoc2-MV"
      },
      "outputs": [],
      "source": [
        "def dql_flappy_bird(model, optimizer, loss_function):\n",
        "\n",
        "    # initiate a game\n",
        "    game = flappy_bird.GameState()\n",
        "\n",
        "    # store the previous state, action and transitions\n",
        "    history_data = deque()\n",
        "\n",
        "    # get the first observation by doing nothing and preprocess the image\n",
        "    do_nothing = np.zeros(num_actions)\n",
        "    do_nothing[0] = 1\n",
        "    image, reward, terminal = game.frame_step(do_nothing)\n",
        "\n",
        "    # preprocess to get the state\n",
        "    current_state = preprocess(image_raw=image)\n",
        "\n",
        "    # training\n",
        "    t = 0\n",
        "\n",
        "    while t < 50000:\n",
        "        if epsilon > np.random.rand(1)[0]:\n",
        "            # random action\n",
        "            action = np.random.choice(num_actions)\n",
        "        else:\n",
        "            # compute the Q function\n",
        "            current_state_tensor = tf.convert_to_tensor(current_state)\n",
        "            current_state_tensor = tf.expand_dims(current_state_tensor, 0)\n",
        "            q_value = model(current_state_tensor, training=False)\n",
        "\n",
        "            # greedy action\n",
        "            #-----MISSING-----#\n",
        "            # your code here\n",
        "            #-----------------#\n",
        "\n",
        "        # take the action and observe the reward and the next state\n",
        "        action_vec = np.zeros([num_actions])\n",
        "        action_vec[action] = 1\n",
        "        image_raw, reward, terminal = game.frame_step(action_vec)\n",
        "        next_state = preprocess(current_state=current_state,\n",
        "                                image_raw=image_raw)\n",
        "\n",
        "        # store the observation\n",
        "        history_data.append((current_state, action, reward, next_state,\n",
        "                            terminal))\n",
        "        if len(history_data) > replay_memory:\n",
        "            history_data.popleft()  # discard old data\n",
        "\n",
        "        # train if done observing\n",
        "        if t > observe:\n",
        "\n",
        "            # sample a batch\n",
        "            batch = random.sample(history_data, batch_size)\n",
        "            state_sample = np.array([d[0] for d in batch])\n",
        "            action_sample = np.array([d[1] for d in batch])\n",
        "            reward_sample = np.array([d[2] for d in batch])\n",
        "            state_next_sample = np.array([d[3] for d in batch])\n",
        "            terminal_sample = np.array([d[4] for d in batch])\n",
        "\n",
        "            # compute the updated Q-values for the samples\n",
        "            #-----MISSING-----#\n",
        "            # your code here\n",
        "            #-----------------#\n",
        "\n",
        "            # train the model on the states and updated Q-values\n",
        "            with tf.GradientTape() as tape:\n",
        "                # compute the current Q-values for the samples\n",
        "                #-----MISSING-----#\n",
        "                # your code here\n",
        "                #-----------------#\n",
        "\n",
        "                # compute the loss\n",
        "                loss = loss_function(updated_q_value, current_q_value)\n",
        "\n",
        "            # backpropagation\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        else:\n",
        "            loss = 0\n",
        "\n",
        "        # update current state and counter\n",
        "        current_state = next_state\n",
        "        t += 1\n",
        "\n",
        "        # print info every 500 steps\n",
        "        if t % 500 == 0:\n",
        "            print(f\"STEP {t} | PHASE {'observe' if t<=observe else 'train'}\",\n",
        "                  f\"| ACTION {action} | REWARD {reward} | LOSS {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDjlS8nd2-MV"
      },
      "source": [
        "You're now ready to play the game! Just run the cell below; do not change the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvwJYrul2-MV"
      },
      "outputs": [],
      "source": [
        "def playgame(start_from_ckpt=False, ckpt_path=None):\n",
        "\n",
        "    #! DO NOT change the random seed !\n",
        "    np.random.seed(4)\n",
        "\n",
        "    if start_from_ckpt:\n",
        "        # if you want to start from a checkpoint\n",
        "        model = keras.models.load_model('ckpt_path')\n",
        "    else:\n",
        "        model = create_q_model()\n",
        "\n",
        "    # specify the optimizer and loss function\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=step_size, clipnorm=1.0)\n",
        "    loss_function = keras.losses.MeanSquaredError()\n",
        "\n",
        "    # play the game\n",
        "    dql_flappy_bird(model=model, optimizer=optimizer, loss_function=loss_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lw-tWBI2-MW"
      },
      "outputs": [],
      "source": [
        "playgame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fi6y_Pq2-MW"
      },
      "source": [
        "### 3.5 Describe the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvZU4Ivw2-MW"
      },
      "source": [
        "Describe what you see by answering the following questions:\n",
        "\n",
        "- In the early stage of training (within 2,000 steps in the *explore* phase),\n",
        "  describe the behavior of the Flappy Bird. What do you think is the greedy policy\n",
        "  given by the estimation of the Q-function in this stage?\n",
        "- Describe what you see after roughly 5,000 training steps.\n",
        "  Do you see any improvement?\n",
        "  In particular, compare Flappy's behavior with their behavior in the early stages of\n",
        "  training.\n",
        "- Explain why the performance has improved, by relating to the model\n",
        "  design such as the replay memory and the exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7607qWuC2-MW"
      },
      "source": [
        "[Your answer here in Markdown]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1rNIHsU2-MW"
      },
      "source": [
        "It takes a long time to fully train the network, so you're not required to\n",
        "complete the training. Here's a [video](https://www.youtube.com/watch?v=THhUXIhjkCM) showing the performance of a well trained DQN."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "IML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}